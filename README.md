# ğŸ¦œï¸ğŸ”— Ragas LLM Evaluation Framework

> **The Ultimate Guide to Testing & Evaluating Custom LLM Applications**

## ğŸ“– Introduction: Why This Project Exists?

### â“ The Problem
Building an LLM app is easy. **Ensuring it works correctly is hard.**
- When you build a **RAG (Retrieval Augmented Generation)** system, how do you know if it retrieved the *right* document?
- How do you know if the LLM isn't **hallucinating** (making things up)?
- How do you measure if the answer is accurate without manually checking 1000s of queries?

### ğŸ’¡ The Solution: Ragas
**Ragas** (Retrieval Augmented Generation Assessment) is a framework that provides **quantitative metrics** to evaluate your LLM pipeline. It's like a "Unit Test" for your AI.

This project implements a complete **Test Automation Framework** for LLMs using **Pytest** and **Ragas**, ensuring your AI is reliable, accurate, and production-ready.

---

## ğŸ—ï¸ Architecture & Testing Scope

We test the entire lifecycle of an AI response, from data retrieval to final generation.

```mermaid
graph LR
    subgraph Data_Pipeline
        A[Proprietary Data] -->|Embed| B[Embedding Model]
        B -->|Store| C[(Vector Database)]
    end
    
    subgraph Retrieval_Phase
        D[User Question] -->|Embed| E[Embedding Model]
        E -->|Search| C
        C -->|Top K Docs| F[Retrieved Context]
    end
    
    subgraph Generation_Phase
        D -->|Input| G[Prompt Template]
        F -->|Input| G
        G -->|Generate| H["LLM (GPT-4)"]
        H -->|Output| I[Final Answer]
    end
    
    style C fill:#f9f,stroke:#333
    style H fill:#bbf,stroke:#333
```

### ğŸ¯ We Test 3 Key Areas:
1.  **Retrieval Module:** Are we finding the right data? (Precision & Recall)
2.  **Generation Module:** Is the LLM answering correctly? (Faithfulness & Relevance)
3.  **End-to-End:** Is the entire system working? (Factual Correctness & Topic Adherence)

---

## ğŸ“Š Evaluation Metrics Explained

We use 7 core metrics to score our LLM. Here is what they mean in simple terms:

| Metric | Phase | What it checks? | Why it matters? |
| :--- | :--- | :--- | :--- |
| **Context Precision** | Retrieval | **Signal-to-Noise Ratio.** Did we retrieve *only* useful documents? | reduces LLM confusion. |
| **Context Recall** | Retrieval | **Completeness.** Did we retrieve *all* the necessary facts? | Ensures no missing info. |
| **Faithfulness** | Generation | **Hallucination Check.** Is the answer derived *only* from the context? | Prevents lying. |
| **Answer Relevance** | Generation | **On-Topic Check.** Did the LLM actually answer the specific question? | Ensures usefulness. |
| **Factual Correctness** | End-to-End | **Truth Check.** Does the answer match the "Ground Truth"? | Ensures accuracy. |
| **Topic Adherence** | Multi-Turn | **Drift Check.** Does the bot stay on topic during a long chat? | Vital for chatbots. |
| **Rubrics Score** | Custom | **Rule Check.** Does the answer meet specific custom rules? | Custom business logic. |

---

## ğŸš€ Key Features

*   **ğŸ§ª Automated Pytest Suite:** Standard software testing practices applied to AI.
*   **ğŸ§¬ Synthetic Data Factory:** Automatically generates test cases from your documents (`fs11/` folder).
*   **ğŸ“ˆ Dashboard (Streamlit):** Visualise pass/fail rates and analyze trends over time.
*   **ğŸ”„ Multi-Turn Support:** Validates conversational history, not just single questions.
*   **âš™ï¸ CI/CD Integration:** Runs automatically on GitHub Actions.

---

## ğŸ› ï¸ Tech Stack

*   **Language:** Python 3.10+
*   **Testing Framework:** Pytest (Asyncio)
*   **Evaluation Engine:** Ragas
*   **LLM Provider:** OpenAI (GPT-4o)
*   **Orchestration:** LangChain
*   **Visualization:** Streamlit, Plotly

---

## âš¡ï¸ Quick Start Guide

### 1ï¸âƒ£ Setup & Install
```bash
git clone https://github.com/abhi9avx/ragas-llm-evaluation.git
cd ragas-llm-evaluation
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
export OPENAI_API_KEY="sk-..."
```

### 2ï¸âƒ£ Run Tests (The "Golden Commands")

| Test File | Description | Command |
| :--- | :--- | :--- |
| `Test1.py` | Check **Retrieval Precision** | `pytest -s Test1.py` |
| `Test2.py` | Check **Retrieval Recall** | `pytest -s Test2.py` |
| `Test3_framework.py` | Run **Parametrized Suite** | `pytest -s Test3_framework.py` |
| `Test4.py` | Check **Faithfulness** | `pytest -s Test4.py` |
| `Test5.py` | Check **Relevance & Accuracy** | `pytest -s Test5.py` |
| `Test6.py` | Check **Topic Adherence** | `pytest -s Test6.py` |
| `Test7.py` | Check **Rubrics Score** | `pytest -s Test7.py` |

### 3ï¸âƒ£ Generate Test Data
Don't have test data? Let AI write it for you!
```bash
# Reads docs from fs11/ and creates 10 test questions
python testDataFeaxtory.py 10
```

### 4ï¸âƒ£ Launch Dashboard
See your results in a professional UI.
```bash
streamlit run dashboard/app.py
```

---

## ğŸ“‚ Project Structure

```text
ragas-llm-evaluation/
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ run_eval.py             # ğŸ§  Main execution engine
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py                  # ğŸ“Š Streamlit dashboard
â”œâ”€â”€ testDataFeaxtory.py         # ğŸ§¬ Synthetic data generator factory
â”œâ”€â”€ results/                    # ğŸ’¾ Stores history of runs (JSON/CSV)
â”œâ”€â”€ fs11/                       # ğŸ“‚ Source documents (PDF/Docx)
â”œâ”€â”€ Test1.py - Test7.py         # ğŸ§ª Individual Test Scripts
â””â”€â”€ requirements.txt            # ğŸ“¦ Dependencies
```

---

## ğŸ”® Upcoming Features

*   **Self-Healing RAG:** Automatically rewrite failing retrieval queries based on low scores.
*   **Cost Analysis:** Track token usage and estimated cost per evaluation run.
*   **Vector DB Integration:** Direct connectors for Pinecone, Weaviate, and Milvus.
*   **Slack/Discord Alerts:** Get notified immediately when your model quality drops.

---

<div align="center">

### Made with â¤ï¸ by [Abhinav (abhi9avx)](https://github.com/abhi9avx) ğŸš€

</div>

